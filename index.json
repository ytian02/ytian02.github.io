[{"categories":["Others"],"collections":["Issue"],"content":" RL4LLM: 整理 DPO、GRPO、Tree-GRPO、GTPO、DAPO、λ\\lambdaλ-GRPO、GSPO、GDPO 的原理和代码实现 III Policy-Based RL: PPO 中使用了两次近似，后续会详细阐述。 IV 离线强化学习: 有空会整理一下 CQL 的详细推导，原文附录中的证明是有问题的，知乎上有相关讨论。 V Maximum Entropy RL: 后续会整理 SAC 的原理和实现。 ","date":"2026-01-20","objectID":"/posts/todolist/:0:0","tags":["Network"],"title":"ToDoList","uri":"/posts/todolist/#"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"基本概念 最大熵强化学习在标准RL的基础上引入了熵正则项，以鼓励策略网络的输出更具多样性。 ","date":"2026-01-19","objectID":"/posts/v-maximum-entropy-rl/:1:0","tags":["RL"],"title":"V Maximum Entropy RL","uri":"/posts/v-maximum-entropy-rl/#基本概念"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"目标 在最大熵强化学习中，学习目标被定义为 J(θ)=ES[Vπ(S)+αH(S;θ)],J(\\theta)=\\mathbb{E}_S\\left[V^\\pi(S)+\\alpha H(S;\\theta)\\right],J(θ)=ES​[Vπ(S)+αH(S;θ)], 其中H(s;θ)=−∑a∈Aπ(a∣s;θ)log⁡π(a∣s;θ)H(s;\\theta)=-\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s;\\theta)\\log\\pi(a\\vert s;\\theta)H(s;θ)=−a∈A∑​π(a∣s;θ)logπ(a∣s;θ)。 ","date":"2026-01-19","objectID":"/posts/v-maximum-entropy-rl/:2:0","tags":["RL"],"title":"V Maximum Entropy RL","uri":"/posts/v-maximum-entropy-rl/#目标"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"常见算法 Soft Q-learning, SAC ","date":"2026-01-19","objectID":"/posts/v-maximum-entropy-rl/:3:0","tags":["RL"],"title":"V Maximum Entropy RL","uri":"/posts/v-maximum-entropy-rl/#常见算法"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"软价值函数 在标准的Q-learning中，状态价值函数V(s)=max⁡a∈AQ(s,a)V(s)=\\max\\limits_{a\\in\\mathcal{A}}Q(s,a)V(s)=a∈Amax​Q(s,a)，而最大化操作是一种硬操作，而在最大熵强化学习的框架下，状态价值函数变成 Vsoft(s)=αlog⁡(∑aexp⁡(Qsoft(s,a)α)),V_\\text{soft}(s)=\\alpha\\log\\left(\\sum\\limits_{a}\\exp\\left(\\dfrac{Q_\\text{soft}(s,a)}{\\alpha}\\right)\\right),Vsoft​(s)=αlog(a∑​exp(αQsoft​(s,a)​)),其中α\\alphaα是温度系数，控制熵正则项的重要性。 ","date":"2026-01-19","objectID":"/posts/v-maximum-entropy-rl/:4:0","tags":["RL"],"title":"V Maximum Entropy RL","uri":"/posts/v-maximum-entropy-rl/#软价值函数"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"详细推导 给定状态sss，最大熵强化学习的优化目标为 max⁡πVsoft(s)=∑a∈Aπ(a∣s)Q(s,a)−α∑a∈Aπ(a∣s)log⁡π(a∣s),s.t. ∑a∈Aπ(a∣s)=1.\\max\\limits_{\\pi}V_\\text{soft}(s)=\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)Q(s,a)-\\alpha\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)\\log\\pi(a\\vert s),\\quad \\text{s.t. }\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)=1.πmax​Vsoft​(s)=a∈A∑​π(a∣s)Q(s,a)−αa∈A∑​π(a∣s)logπ(a∣s),s.t. a∈A∑​π(a∣s)=1.利用拉格朗日乘子法引入乘子λ\\lambdaλ构建拉格朗日函数L\\mathcal{L}L L(π,λ)=∑a∈Aπ(a∣s)(Q(s,a)−αlog⁡π(a∣s))−λ(∑a∈Aπ(a∣s)−1),\\mathcal{L}(\\pi,\\lambda)=\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)\\left(Q(s,a)-\\alpha\\log\\pi(a\\vert s)\\right)-\\lambda(\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)-1),L(π,λ)=a∈A∑​π(a∣s)(Q(s,a)−αlogπ(a∣s))−λ(a∈A∑​π(a∣s)−1),通过对π(a∣s)\\pi(a\\vert s)π(a∣s)求偏导可得 ∂L(π,λ)∂π(a∣s)=Q(s,a)−α−αlog⁡π(a∣s)−λ=0,\\dfrac{\\partial\\mathcal{L}(\\pi,\\lambda)}{\\partial\\pi(a\\vert s)}=Q(s,a)-\\alpha-\\alpha\\log\\pi(a\\vert s)-\\lambda=0,∂π(a∣s)∂L(π,λ)​=Q(s,a)−α−αlogπ(a∣s)−λ=0,求解可得 π(a∣s)=exp⁡(Q(s,a)α)⋅(−λα−1),\\pi(a\\vert s)=\\exp\\left(\\dfrac{Q(s,a)}{\\alpha}\\right)\\cdot\\left(-\\dfrac{\\lambda}{\\alpha}-1\\right),π(a∣s)=exp(αQ(s,a)​)⋅(−αλ​−1),由于∑a∈Aπ(a∣s)=1\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)=1a∈A∑​π(a∣s)=1，可以通过归一化来消除常数项，进而得到最优策略 π(a∣s)=exp⁡(Q(s,a)α)∑a′∈Aexp⁡(Q(s,a′)α)\\pi(a\\vert s)=\\dfrac{\\exp\\left(\\dfrac{Q(s,a)}{\\alpha}\\right)}{\\sum\\limits_{a^\\prime\\in\\mathcal{A}}\\exp\\left(\\dfrac{Q(s,a^\\prime)}{\\alpha}\\right)}π(a∣s)=a′∈A∑​exp(αQ(s,a′)​)exp(αQ(s,a)​)​这是一个关于Q值的Boltzmann分布，将最优策略代入优化目标可得 Vsoft(s)=∑a∈Aπ(a∣s)⋅[Q(s,a)−αlog⁡exp⁡(Q(s,a)α)Z],=α∑a∈Aπ(a∣s)log⁡Z,=αlog⁡∑a′∈Aexp⁡(Q(s,a′)α),\\begin{align} V_\\text{soft}(s)\u0026=\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)\\cdot \\left[Q(s,a)-\\alpha\\log \\dfrac{\\exp\\left(\\dfrac{Q(s,a)}{\\alpha}\\right)}{Z}\\right],\\\\ \u0026=\\alpha\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)\\log Z,\\\\ \u0026=\\alpha\\log\\sum\\limits_{a^\\prime\\in\\mathcal{A}}\\exp\\left(\\dfrac{Q(s,a^\\prime)}{\\alpha}\\right), \\end{align}Vsoft​(s)​=a∈A∑​π(a∣s)⋅​Q(s,a)−αlogZexp(αQ(s,a)​)​​,=αa∈A∑​π(a∣s)logZ,=αloga′∈A∑​exp(αQ(s,a′)​),​​这也就是软价值函数的定义。 ","date":"2026-01-19","objectID":"/posts/v-maximum-entropy-rl/:5:0","tags":["RL"],"title":"V Maximum Entropy RL","uri":"/posts/v-maximum-entropy-rl/#详细推导"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"未完待续 后续会整理SAC的原理和实现。 ","date":"2026-01-19","objectID":"/posts/v-maximum-entropy-rl/:6:0","tags":["RL"],"title":"V Maximum Entropy RL","uri":"/posts/v-maximum-entropy-rl/#未完待续"},{"categories":["Algorithms"],"collections":["LLM"],"content":"PPO 本节主要梳理PPO-Clip算法原理和代码实现。引用GRPO论文中的记法，有PPO的优化目标 JPPO(θ)=Eq∼P(Q)Eo∼πθold(O∣q)1∣o∣∑i=1∣o∣min⁡{π(ot∣q,o\u003ct)πθold(ot∣q,o\u003ct)At,clip(π(ot∣q,o\u003ct)πθold(ot∣q,o\u003ct),1−ϵ,1+ϵ)At},\\mathcal{J}_\\text{PPO}(\\theta)=\\mathbb{E}_{q\\sim P(Q)}\\mathbb{E}_{o\\sim\\pi_{\\theta_\\text{old}}(O\\vert q)}\\dfrac{1}{\\vert o\\vert}\\sum\\limits_{i=1}^{\\vert o\\vert}\\min\\left\\{\\dfrac{\\pi(o_t\\vert q,o_{\u003ct})}{\\pi_{\\theta_\\text{old}}(o_t\\vert q,o_{\u003ct})}A_t,\\text{clip}\\left(\\dfrac{\\pi(o_t\\vert q,o_{\u003ct})}{\\pi_{\\theta_\\text{old}}(o_t\\vert q,o_{\u003ct})},1-\\epsilon,1+\\epsilon\\right)A_t\\right\\},JPPO​(θ)=Eq∼P(Q)​Eo∼πθold​​(O∣q)​∣o∣1​i=1∑∣o∣​min{πθold​​(ot​∣q,o\u003ct​)π(ot​∣q,o\u003ct​)​At​,clip(πθold​​(ot​∣q,o\u003ct​)π(ot​∣q,o\u003ct​)​,1−ϵ,1+ϵ)At​},其中clip(x,l,r)≔max⁡{min⁡{x,r},l}\\text{clip}(x,l,r)\\coloneqq\\max\\left\\{\\min\\left\\{x,r\\right\\},l\\right\\}clip(x,l,r):=max{min{x,r},l}，ϵ\u003e0\\epsilon\u003e0ϵ\u003e0是一个超参数，当优势大于0时，说明动作价值高于平均，最大化这个式子会增大比值，但不会超过1+ϵ1+\\epsilon1+ϵ；当优势小于0时，说明动作价值低于平均，则最大化u这个式子会缩小比值，但不会低于1−ϵ1-\\epsilon1−ϵ。 在稠密奖励的环境中，PPO的实现如下所示（引用动手学强化学习 - PPO算法的实现） import torch import torch.nn as nn import torch.nn.functional as F # Actor网络，用于近似策略分布 class Actor(nn.Module): def __init__(self, state_dim, hidden_dim, action_dim): super().__init__() self.fc1 = nn.Linear(state_dim, hidden_dim) self.act = nn.ReLU() self.fc2 = nn.Linear(hidden_dim, action_dim) def forward(self, state): X = self.act(self.fc1(state)) return torch.softmax(self.fc2(X), dim=-1) # Critic网络，用于近似价值函数 class VCritic(nn.Module): def __init__(self, state_dim, hidden_dim): super().__init__() self.fc1 = nn.Linear(state_dim, hidden_dim) self.act = nn.ReLU() self.fc2 = nn.Linear(hidden_dim, 1) def forward(self, state): X = self.act(self.fc1(state)) return self.fc2(X) class PPO: def __init__(self, state_dim, hidden_dim, action_dim, critic_lr, actor_lr, epochs, lmbda, gamma, eps): self.actor = Actor(state_dim, hidden_dim, action_dim) self.critic = VCritic(state_dim, hidden_dim) self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=actor_lr) self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=critic_lr) self.epochs = epochs self.lmbda = lmbda self.gamma = gamma self.eps = eps def take_action(self, state): probs = self.actor(state) action_dist = torch.distributions.Categorical(probs) action = action_dist.sample() return action.item() def update(self, transition_dict): # (L, state_dim) states = torch.tensor(transition_dict['state'], dtype=torch.float) # (L, 1) actions = torch.tensor(transition_dict['actions'], dtype=torch.float) # (L, 1) rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float) # (L, state_dim) next_states = torch.tensor(transition_dict['next_state'], dtype=torch.float) # (L, 1) dones = torch.tensor(transition_dict['done'], dtype=torch.float) # (L, 1) td_target = rewards + self.critic(next_states) * (1-dones) # (L, 1) td_delta = td_target - self.critic(states) # (L, 1) advs = compute_advantages(td_delta, self.lmbda, self.gamma) # (L, action_dim) old_log_probs = torch.log(self.actor(states)).gather(1, actions).detach() for _ in range(self.epochs): # (L, 1) log_probs = torch.log(self.actor(states)).gather(1, actions) # (L, 1) ratio = torch.exp(log_probs - old_log_probs) # (L, 1) surr1 = ratio * advs # (L, 1) surr2 = torch.clamp(ratio, 1-self.eps, 1+self.eps) * advs # (L, 1) actor_loss = torch.mean(-torch.min(surr1, surr2)) critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach())) self.actor_opt.zero_grad() self.critic_opt.zero_grad() actor_loss.backward() critic_loss.backward() self.actor_opt.step() self.critic_opt.step() def compute_advantages(td_delta: torch.Tensor, lmbda, gamma): # (L, 1) td_delta = td_delta.squeeze(1).detach().numpy() advantages = [] advantage = 0 for delta in td_delta[::-1]: advantage = advantage * gamma * lmbda + delta advantages.append(advantage) advantages.reverse() return torch.tensor(advantages, dtype=torch.float) ","date":"2026-01-19","objectID":"/posts/rl4llm/:1:0","tags":["LLM","RL"],"title":"RL4LLM","uri":"/posts/rl4llm/#ppo"},{"categories":["Algorithms"],"collections":["LLM"],"content":"GRPO Hands-On-Large-Language-Models-CN - Agentic RAG Transformer Reinforcement Learning (TRL) ","date":"2026-01-19","objectID":"/posts/rl4llm/:2:0","tags":["LLM","RL"],"title":"RL4LLM","uri":"/posts/rl4llm/#grpo"},{"categories":["Algorithms"],"collections":["LLM"],"content":"未完待续 后续会整理DPO、GRPO、Tree-GRPO、GTPO、DAPO、λ\\lambdaλ-GRPO、GSPO、GDPO的原理和代码实现（如果有开源实现）。 ","date":"2026-01-19","objectID":"/posts/rl4llm/:3:0","tags":["LLM","RL"],"title":"RL4LLM","uri":"/posts/rl4llm/#未完待续"},{"categories":["Others"],"collections":["Quick Start"],"content":"Hugo for Windows 原教程Windows下使用hugo和Github Pages配置博客，写得非常详细，这里只是粗略地总结了整体的步骤和常用命令。 ","date":"2026-01-19","objectID":"/posts/hugo/:1:0","tags":["Website"],"title":"Hugo","uri":"/posts/hugo/#hugo-for-windows"},{"categories":["Others"],"collections":["Quick Start"],"content":"安装和配置 假设在D:/personal_websites存储个人网站相关内容 # Git Bash Here，运行下面命令创建“blog” hugo new site blog # 安装Fixlt主题 cd blog git init git submodule add https://github.com/hugo-fixit/FixIt.git themes/FixIt # 基础配置 # 修改hugo.toml文件 # 覆盖archetypes/default.md 以及 archetypes/posts.md文件 # 创建文章测试实例 hugo new posts/test.md # 本地调试 hugo serve -D --disableFastRender # 创建blog仓库 git branch -m master main # 重命名分支，可能会用到 git add . git commit -m \"init blog files\" git remote add origin https://github.com/ytian02/blog.git git pull origin main --allow-unrelated-histories # 拉取远程更改并合并，可能会用到 git push -u origin main # 创建 Github Pages 公开仓库 # 上传页面 hugo cd public git init git remote add origin https://github.com/ytian02/ytian02.github.io.git # 将本地目录链接到远程服务器的代码仓库 git add . git commit -m \"[介绍，随便写点什么，比如日期]\" git push -u origin master # Github Action 自动发布 cd ../ mkdir .github mkdir .github/workflows touch .github/workflows/deploy.yml git add . git commit -m \"add action config\" git push ","date":"2026-01-19","objectID":"/posts/hugo/:1:1","tags":["Website"],"title":"Hugo","uri":"/posts/hugo/#安装和配置"},{"categories":["Others"],"collections":["Quick Start"],"content":"创建新blog并上传 # 在站点目录下，打开Git Bash hugo new posts/文章名.md # 站点目录下，新建文章 hugo serve -D --disableFastRender # 使用VScode编辑文章内容或修改，包括修改主题之类的。在本地进行调试 hugo # 修改完成，确定要上传到 GitHub 上后，站点目录下执行 # 进行编译，没错误的话修改的内容就顺利同步到public下了，然后执行提交命令 git add . git commit -m \"随便写点提交信息\" git push ","date":"2026-01-19","objectID":"/posts/hugo/:1:2","tags":["Website"],"title":"Hugo","uri":"/posts/hugo/#创建新blog并上传"},{"categories":["Others"],"collections":["Quick Start"],"content":"常用命令 # 重命名分支 git branch -m master main # 或者 git push -u origin master # 合并远程文件（推荐，更安全） git pull origin main --allow-unrelated-histories # 删除远程分支 git push origin --delete master ","date":"2026-01-19","objectID":"/posts/hugo/:1:3","tags":["Website"],"title":"Hugo","uri":"/posts/hugo/#常用命令"},{"categories":["Others"],"collections":["Quick Start"],"content":"Clash for Linux ","date":"2026-01-19","objectID":"/posts/proxy/:1:0","tags":["Network"],"title":"Proxy","uri":"/posts/proxy/#clash-for-linux"},{"categories":["Others"],"collections":["Quick Start"],"content":"下载和安装 wget https://github.com/doreamon-design/clash/releases/download/v2.0.24/clash_2.0.24_linux_amd64.tar.gz # 下载clash tar zxvf clash_2.0.24_linux_amd64.tar.gz # 解压 chmod +x clash sudo mv clash /usr/local/bin/clash # 移动 clash -v # clash版本 sudo mkdir /etc/clash # 创建目录 cd /etc/clash sudo wget -O config.yaml https:// # 导入订阅 sudo mv /home/ubuntu/Country.mmdb /etc/clash/ # GeoIP功能 ","date":"2026-01-19","objectID":"/posts/proxy/:1:1","tags":["Network"],"title":"Proxy","uri":"/posts/proxy/#下载和安装"},{"categories":["Others"],"collections":["Quick Start"],"content":"启动 sudo clash -d /etc/clash # 启动 启动后可使用 http://clash.razord.top/ 调整节点 ","date":"2026-01-19","objectID":"/posts/proxy/:1:2","tags":["Network"],"title":"Proxy","uri":"/posts/proxy/#启动"},{"categories":["Others"],"collections":["Quick Start"],"content":"临时代理 临时代理仅在临时窗口生效 export http_proxy=\"http://127.0.0.1:7890\" export https_proxy=\"http://127.0.0.1:7890\" curl ipinfo.io # 测试代理 env | grep -i proxy # 查看代理设置 unset http_proxy # 删除代理设置 unset https_proxy # 删除代理设置 sudo lsof -i :9090 sudo kill -9 pid # 删除存在的进程 ","date":"2026-01-19","objectID":"/posts/proxy/:1:3","tags":["Network"],"title":"Proxy","uri":"/posts/proxy/#临时代理"},{"categories":["Others"],"collections":["Quick Start"],"content":"Git Bash for windows # 取消旧代理 git config --global --unset http.proxy git config --global --unset https.proxy # 设置代理 # 请将 7890 替换为您实际的代理端口 git config --global http.proxy http://127.0.0.1:7890 git config --global https.proxy http://127.0.0.1:7890 ","date":"2026-01-19","objectID":"/posts/proxy/:2:0","tags":["Network"],"title":"Proxy","uri":"/posts/proxy/#git-bash-for-windows"},{"categories":["Others"],"collections":["Quick Start"],"content":"Hands-on-LLMs conda create -n rl4llm python=3.10 conda activate rl4llm pip install torch==2.6.0 --index-url https://download.pytorch.org/whl/cu124 pip install transformers pip install pandas matplotlib scikit-learn tqdm ipykernel python -m ipykernel install --user --name rl4llm --display-name \"rl4llm\" ","date":"2026-01-19","objectID":"/posts/environment-settings/:1:0","tags":["python"],"title":"Environment Settings","uri":"/posts/environment-settings/#hands-on-llms"},{"categories":["Others"],"collections":["Quick Start"],"content":"DRL4Long-termEngagement conda create -n DRL4LE python=3.9.13 conda activate DRL4LE pip install torch==2.3.0 --index-url https://download.pytorch.org/whl/cu121 pip install pandas matplotlib scikit-learn tqdm ipykernel torchtest wordcloud svglib reportlab python -m ipykernel install --user --name DRL4LE --display-name \"DRL4LE\" pip install numpy==1.24.3 ","date":"2026-01-19","objectID":"/posts/environment-settings/:2:0","tags":["python"],"title":"Environment Settings","uri":"/posts/environment-settings/#drl4long-termengagement"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"基本概念 区别于在线(同策略)强化学习和异策略强化学习方法，离线强化学习从离线的经验回放数组中直接学习一个策略用于和环境交互。 “Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems” (Levine et al., 2023) ","date":"2026-01-19","objectID":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/:1:0","tags":["RL"],"title":"IV 离线强化学习","uri":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/#基本概念"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"挑战 (策略分布偏移) 策略的改进往往需要探索新的动作，但是离线数据集并没有真实动作的反事实估计，因此在对未知动作的较高的值估计会导致策略分布偏移真实动作空间。 ","date":"2026-01-19","objectID":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/:2:0","tags":["RL"],"title":"IV 离线强化学习","uri":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/#挑战"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"现有范式 现有的Offline RL有两个范式，以权衡策略改进和策略分布偏移 Behavior Constrained Policy Optimization: 显式地使用目标策略和行为策略之间的散度作为正则化； Conservative Methods: 惩罚超出分布（out-of-distribution, OOD）的动作的价值估计，以避免高估目标策略价值的错误 “Offline reinforcement learning with closed-form policy improvement operators” (Liu et al., ICLR, 2023) ","date":"2026-01-19","objectID":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/:3:0","tags":["RL"],"title":"IV 离线强化学习","uri":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/#现有范式"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"经典方法 ","date":"2026-01-19","objectID":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/:4:0","tags":["RL"],"title":"IV 离线强化学习","uri":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/#经典方法"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"BCQ (Batch-Constrained deep Q-learning) 使用条件变分自编码器生成与离线数据集中出现过的动作相似的候选动作，并在一定的范围内进行扰动以确保动作的多样性，通过类似q-learning的方式来学习最优策略。 算法流程: 批量样本抽样；VAE编码 (s,a)(s,a)(s,a) 再解码出 a~\\tilde{a}a~；计算重构误差和KL散度以优化VAE的参数；从VAE中抽样得到状态 s′s^\\primes′ 的 nnn 个候选动作并加上扰动；计算TD target并更新动作价值网络。 注意BCQ使用的是CVAE (Conditional VAE)，CVAE建模的是条件生成分布，在训练时，编码器以数据和条件作为输入输出近似后验分布的参数，解码器以抽样得到的隐变量和条件作为输入输出重建数据；在推理时，根据抽样得到的隐变量和条件生成新的数据，以数字9为例，条件是9，通过调整隐变量可以生成各种风格的数字9。 “Off-policy deep reinforcement learning without exploration” (Fujimoto, ICML, 2019)[Code] ","date":"2026-01-19","objectID":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/:4:1","tags":["RL"],"title":"IV 离线强化学习","uri":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/#bcq-batch-constrained-deep-q-learning"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"CQL (Constrained Q-Learning) 在标准的贝尔曼误差（TD误差）前加上了CQL正则项，惩罚了分布外动作的Q值，奖励服从离线数据集中动作分布的Q值 Q^k+1←arg⁡min⁡Qα⋅(Es∼D,a∼μ(a∣s)[Q(s,a)]−Es∼D,a∼π^β(a∣s)[Q(s,a)])+12Es,a,s′∼D[(Q(s,a)−B^πQ^k(s,a))2]. \\hat{Q}^{k+1}\\leftarrow\\arg\\min\\limits_{Q}\\alpha\\cdot\\left(\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\mu(a\\vert s)}\\left[Q(s,a)\\right]-\\textcolor{red}{\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\hat{\\pi}_\\beta(a\\vert s)}\\left[Q(s,a)\\right]}\\right)+\\dfrac{1}{2}\\mathbb{E}_{s,a,s^\\prime\\sim\\mathcal{D}}\\left[\\left(Q(s,a)-\\hat{\\mathcal{B}}^\\pi\\hat{Q}^k(s,a)\\right)^2\\right].Q^​k+1←argQmin​α⋅(Es∼D,a∼μ(a∣s)​[Q(s,a)]−Es∼D,a∼π^β​(a∣s)​[Q(s,a)])+21​Es,a,s′∼D​[(Q(s,a)−B^πQ^​k(s,a))2]. 其中Bπ\\mathcal{B}^\\piBπ是贝尔曼算子(Bellman operator)，B^πQ^k(s,a)\\hat{\\mathcal{B}}^\\pi\\hat{Q}^k(s,a)B^πQ^​k(s,a)其实就是TD target。上式进一步可以改写为 min⁡Qmax⁡μα⋅(Es∼D,a∼μ(a∣s)[Q(s,a)]−Es∼D,a∼π^β(a∣s)[Q(s,a)])+12Es,a,s′∼D[(Q(s,a)−B^πQ^k(s,a))2]+R(μ)(CQL(R)).\\min\\limits_{Q}\\textcolor{red}{\\max\\limits_\\mu}\\alpha\\cdot\\left(\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\textcolor{red}{\\mu(a\\vert s)}}\\left[Q(s,a)\\right]-\\textcolor{red}{\\mathbb{E}_{s\\sim\\mathcal{D},a\\sim\\hat{\\pi}_\\beta(a\\vert s)}\\left[Q(s,a)\\right]}\\right)+\\dfrac{1}{2}\\mathbb{E}_{s,a,s^\\prime\\sim\\mathcal{D}}\\left[\\left(Q(s,a)-\\hat{\\mathcal{B}}^\\pi\\hat{Q}^k(s,a)\\right)^2\\right]+\\textcolor{red}{\\mathcal{R}(\\mu)}\\quad(\\text{CQL}(\\mathcal{R})).Qmin​μmax​α⋅(Es∼D,a∼μ(a∣s)​[Q(s,a)]−Es∼D,a∼π^β​(a∣s)​[Q(s,a)])+21​Es,a,s′∼D​[(Q(s,a)−B^πQ^​k(s,a))2]+R(μ)(CQL(R)).其中R(μ)\\mathcal{R}(\\mu)R(μ)是一个正则化器。令R(μ)=−DKL(μ∥ρ)\\mathcal{R}(\\mu)=-D_\\text{KL}(\\mu\\Vert\\rho)R(μ)=−DKL​(μ∥ρ)，其中ρ(a∣s)\\rho(a\\vert s)ρ(a∣s)是先验分布，推导出μ(a∣s)∝ρ(a∣s)⋅exp⁡(Q(s,a))\\mu(a\\vert s)\\propto\\rho(a\\vert s)\\cdot\\exp(Q(s,a))μ(a∣s)∝ρ(a∣s)⋅exp(Q(s,a))，当ρ=Unif(a)\\rho=\\text{Unif}(a)ρ=Unif(a)时，可以推导出如下变体，记为CQL(H)\\text{CQL}(\\mathcal{H})CQL(H) min⁡QαEs∼D[log⁡∑a∈Aexp⁡(Q(s,a))−Ea∼π^β(a∣s)[Q(s,a)]]+12Es,a,s′∼D[(Q(s,a)−B^πQ^k(s,a))2].\\min\\limits_{Q}\\alpha\\mathbb{E}_{s\\sim\\mathcal{D}}\\left[\\log\\sum\\limits_{a\\in\\mathcal{A}}\\exp(Q(s,a))-{\\mathbb{E}_{a\\sim\\hat{\\pi}_\\beta(a\\vert s)}\\left[Q(s,a)\\right]}\\right]+\\dfrac{1}{2}\\mathbb{E}_{s,a,s^\\prime\\sim\\mathcal{D}}\\left[\\left(Q(s,a)-\\hat{\\mathcal{B}}^\\pi\\hat{Q}^k(s,a)\\right)^2\\right].Qmin​αEs∼D​[loga∈A∑​exp(Q(s,a))−Ea∼π^β​(a∣s)​[Q(s,a)]]+21​Es,a,s′∼D​[(Q(s,a)−B^πQ^​k(s,a))2].如果ρ=π^k−1\\rho=\\hat{\\pi}^{k-1}ρ=π^k−1，则(4)的第一项被π^k−1\\hat{\\pi}^{k-1}π^k−1选中的动作的Q值的指数加权平均替代。注意到(4)式的更新不包括策略，可以直接令μ∗\\mu^\\astμ∗为策略；如果使用Actor-Critic的方式，需要借助SAC算法额外训练Actor。一些额外的资料离线强化学习(Offline RL)系列3: (算法篇) CQL 算法详解与实现 - 知乎CQL算法logsumexp公式推导 - 知乎Conservative Q Learning(保守强化学习)傻瓜级讲解和落地教程 - 知乎 “Conservative q-learning for offline reinforcement learning” (Kumar et al., NeurIPS, 2020)[Code] ","date":"2026-01-19","objectID":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/:4:2","tags":["RL"],"title":"IV 离线强化学习","uri":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/#cql-constrained-q-learning"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"TD3+BC (Twin Delayed Deep Deterministic Gradiant with Behavior Cloning) 在TD3算法中的更新策略步骤添加了行为克隆损失项 “A Minimalist Approach to Ofﬂine Reinforcement Learning” (Fujimoto, NeurIPS, 2021)[Code] ","date":"2026-01-19","objectID":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/:4:3","tags":["RL"],"title":"IV 离线强化学习","uri":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/#td3bc-twin-delayed-deep-deterministic-gradiant-with-behavior-cloning"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"未完待续 有空会整理一下CQL的详细推导，原文附录中的证明是有问题的，知乎上有相关讨论。 ","date":"2026-01-19","objectID":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/:5:0","tags":["RL"],"title":"IV 离线强化学习","uri":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/#未完待续"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"神秘问题 不知道为什么只要标题出现\"Offline RL\"的字样就会出现公式双重显示的问题，所以本节使用中文标题。 ","date":"2026-01-19","objectID":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/:6:0","tags":["RL"],"title":"IV 离线强化学习","uri":"/posts/iv-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/#神秘问题"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"本文梳理了基于策略的强化学习算法。","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"基本概念 策略学习方法关注如何近似最优策略，优化目标是J(θ)=ES[Vπ(S)]J(\\theta)=\\mathbb{E}_S\\left[V^\\pi(S)\\right]J(θ)=ES​[Vπ(S)]，其中θ\\thetaθ是策略π\\piπ的参数。 ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:1:0","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#基本概念"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"策略梯度公式 ∂J(θ)∂θ=ES[EA∼π(⋅∣S;θ)[∂log⁡π(A∣S;θ)∂θ⋅Qπ(S,A)]].\\dfrac{\\partial J(\\theta)}{\\partial\\theta}=\\mathbb{E}_{S}\\left[\\mathbb{E}_{A\\sim\\pi(\\cdot\\vert S;\\theta)}\\left[\\dfrac{\\partial\\log\\pi(A\\vert S;\\theta)}{\\partial\\theta}\\cdot Q^\\pi(S,A)\\right]\\right].∂θ∂J(θ)​=ES​[EA∼π(⋅∣S;θ)​[∂θ∂logπ(A∣S;θ)​⋅Qπ(S,A)]].事实上，对状态的期望前面应该还有系数1−γn1−γ\\dfrac{1-\\gamma^n}{1-\\gamma}1−γ1−γn​，但在实际应用中，该系数可以忽略，这是因为在做梯度上升时，系数会被学习率吸收。此外，策略梯度定理只有在状态SSS服从马尔可夫链的稳态分布d(⋅)d(\\cdot)d(⋅)时成立。注：其实有点像使用Q值加权策略的梯度上升过程。 ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:2:0","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#策略梯度公式"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"广义优势估计(Generalized Advantage Estimation, GAE) 参考六、GAE 广义优势估计 - 知乎整理 ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:3:0","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#广义优势估计generalized-advantage-estimation-gae"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"背景 在Policy-based RL中，经常引入baseline Vπ(s)V^\\pi(s)Vπ(s) 来计算优势值函数 Aπ(s,a)=Qπ(s,a)−Vπ(s)=Es′∼P(s′∣s,a)(r(s,a)+γVπ(s′)−Vπ(s)),A^\\pi(s,a)=Q^\\pi(s,a)-V^\\pi(s)=\\mathbb{E}_{s^\\prime \\sim P(s^\\prime\\vert s,a)}\\left(r(s,a)+\\gamma V^\\pi(s^\\prime)-V^\\pi(s)\\right),Aπ(s,a)=Qπ(s,a)−Vπ(s)=Es′∼P(s′∣s,a)​(r(s,a)+γVπ(s′)−Vπ(s)),由于使用神经网络Vθ(s)V_\\theta(s)Vθ​(s)近似值函数时存在偏差，因此在使用损失函数Es,a,s′[r(s,a)+γVθ(s′)−Vθ(s)]2\\mathbb{E}_{s,a,s^\\prime}\\left[r(s,a)+\\gamma V_\\theta(s^\\prime)-V_\\theta(s)\\right]^2Es,a,s′​[r(s,a)+γVθ​(s′)−Vθ​(s)]2（TD方法）优化优势值估计是一种高偏差、低方差（只涉及单步值估计）的方法，考虑到状态价值函数的定义 Vπ(st)=Eπ[∑k=0nγkRt+k∣S=s],V^\\pi(s_t)=\\mathbb{E}_\\pi[\\sum\\limits_{k=0}^n\\gamma^kR_{t+k}\\vert S=s],Vπ(st​)=Eπ​[k=0∑n​γkRt+k​∣S=s],也可以考虑一局游戏结束后使用∑k=0nγkrt+k\\sum\\limits_{k=0}^n\\gamma^k r_{t+k}k=0∑n​γkrt+k​对状态价值函数进行近似（MC方法），进而求解优势值，这是一种低偏差、高方差（涉及多步奖励）的方法。为了结合TD和MC的特性，n步TD方法考虑了n步回报 Gt ⁣:t+n=Rt+γRt+1+⋯+γn−1Rt+n−1+γnVπ(St+n),G_{t\\colon t+n}=R_t+\\gamma R_{t+1}+\\cdots+\\gamma^{n-1}R_{t+n-1}+\\gamma^nV^\\pi(S_{t+n}),Gt:t+n​=Rt​+γRt+1​+⋯+γn−1Rt+n−1​+γnVπ(St+n​),λ-return方法进行了加权平均，平衡了偏差和方差，考虑有限步TTT结束游戏， Gtλ=(1−λ)∑n=1T−t−1λn−1Gt ⁣:t+n+λT−t−1Gt,G^\\lambda_t=(1-\\lambda)\\sum\\limits_{n=1}^{T-t-1}\\lambda^{n-1}G_{t\\colon t+n}+\\lambda^{T-t-1}G_{t},Gtλ​=(1−λ)n=1∑T−t−1​λn−1Gt:t+n​+λT−t−1Gt​,其中GtG_tGt​是折扣回报。当λ=0\\lambda=0λ=0时，退化为n步TD方法；当λ=1\\lambda=1λ=1时，退化为MC方法。对于无限步，则有 Gtλ=(1−λ)∑n=1∞λn−1Gt:t+n.G_t^\\lambda=(1-\\lambda)\\sum\\limits_{n=1}^\\infty\\lambda^{n-1}G_{t:t+n}.Gtλ​=(1−λ)n=1∑∞​λn−1Gt:t+n​.","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:3:1","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#背景"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"GAE GAE借鉴了λ-return方法的思想，考虑优势函数的定义，有 A^t1=rt+γV(st+1)−V(st)=δtA^t2=rt+γrt+1+γ2V(st+2)−V(st)=δt+γδt+1⋯A^tn=rt+γrt+1+⋯+γn−1rt+n−1+γnV(st+n)−V(st)=∑k=1nγk−1δt+k−1⋯A^t∞=∑l=0∞γlrt+l−V(st)=∑l=0∞γlδt+l\\begin{align}\u0026\\hat{A}_t^1=r_t+\\gamma V(s_{t+1})-V(s_t)=\\delta_t\\\\\u0026\\hat{A}_t^2=r_{t}+\\gamma r_{t+1}+\\gamma^2 V(s_{t+2})-V(s_t)=\\delta_t+\\gamma\\delta_{t+1}\\\\\u0026\\cdots\\\\\u0026\\hat{A}_t^n=r_{t}+\\gamma r_{t+1}+\\cdots+\\gamma^{n-1}r_{t+n-1}+\\gamma^n V(s_{t+n})-V(s_t)=\\sum\\limits_{k=1}^n\\gamma^{k-1}\\delta_{t+k-1}\\\\\u0026\\cdots\\\\\u0026\\hat{A}_t^\\infty=\\sum\\limits_{l=0}^\\infty\\gamma^{l}r_{t+l}-V(s_t)=\\sum\\limits_{l=0}^\\infty\\gamma^l\\delta_{t+l}\\end{align}​A^t1​=rt​+γV(st+1​)−V(st​)=δt​A^t2​=rt​+γrt+1​+γ2V(st+2​)−V(st​)=δt​+γδt+1​⋯A^tn​=rt​+γrt+1​+⋯+γn−1rt+n−1​+γnV(st+n​)−V(st​)=k=1∑n​γk−1δt+k−1​⋯A^t∞​=l=0∑∞​γlrt+l​−V(st​)=l=0∑∞​γlδt+l​​​参考λ-return方法，考虑无限步的推导，有 A^tGAE(λ,γ)=(1−λ)∑n=1∞λn−1A^tn=∑l=0∞(γλ)lδt+l=∑l=1∞(γλ)lδt+l+δt=γλA^t+1GAE(λ,γ)+δt\\begin{align}\\hat{A}_t^{\\text{GAE}(\\lambda,\\gamma)}\u0026=(1-\\lambda)\\sum\\limits_{n=1}^{\\infty}\\lambda^{n-1}\\hat{A}_t^n\\\\\u0026=\\sum\\limits_{l=0}^\\infty(\\gamma\\lambda)^l\\delta_{t+l}\\\\\u0026=\\sum\\limits_{l=1}^\\infty(\\gamma\\lambda)^l\\delta_{t+l}+\\delta_t\\\\\u0026=\\gamma\\lambda\\hat{A}_{t+1}^{\\text{GAE}(\\lambda,\\gamma)}+\\delta_{t}\\end{align}A^tGAE(λ,γ)​​=(1−λ)n=1∑∞​λn−1A^tn​=l=0∑∞​(γλ)lδt+l​=l=1∑∞​(γλ)lδt+l​+δt​=γλA^t+1GAE(λ,γ)​+δt​​​GAE详细的推导过程也可以参考11.6 广义优势估计，写得也很清楚。 ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:3:2","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#gae"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"Policy-based RL ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:4:0","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#policy-based-rl"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"Reinforce 通过策略梯度和（回报的）蒙特卡洛近似utu_tut​直接优化策略网络π(a∣s;θ)\\pi(a\\vert s;\\theta)π(a∣s;θ) Simple statistical gradient-following algorithms for connectionist reinforcement learning (Williams, ML, 1992)[Code] ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:4:1","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#reinforce"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"Reinforce with baseline 引入了状态价值网络v(s;ω)v(s;\\omega)v(s;ω)作为基线，状态价值网络使用utu_tut​作为监督信号来训练。 (论文引用同上) ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:4:2","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#reinforce-with-baseline"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"Actor-Critic 使用策略梯度优化策略网络π(a∣s;θ)\\pi(a\\vert s;\\theta)π(a∣s;θ)(actor)，TD算法优化动作价值网络qω(s,a;ω)q_\\omega(s,a;\\omega)qω​(s,a;ω)(critic) Actor-Critic Algorithms (Konda, NeurIPS, 1999)[Code] 注: 也有说法认为Actor-Critic方法起源于Neuronlike adaptive elements that can solve difficult learning control problems (Barto et al., Tos, 1983),或Policy Gradient Methods for Reinforcement Learning with Function Approximation (Sutton et al., NeurIPS, 1999) ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:4:3","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#actor-critic"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"A2C (Advantage Actor-Critic) 引入状态价值网络v(s;ω)v(s;\\omega)v(s;ω)作为基线，使用TD算法更新v(ω)v(\\omega)v(ω)，并根据v(s;ω)v(s;\\omega)v(s;ω)计算的TD误差和策略梯度更新策略网络π(a∣s;θ)\\pi(a\\vert s;\\theta)π(a∣s;θ) OpenAI Baselines: ACKTR \u0026 A2C (OpenAI, 2017)[Code] ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:4:4","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#a2c-advantage-actor-critic"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"TRPO (Trust Region Policy Optimization) (置信域算法的思想) 找一个在当前参数θnow\\theta_\\text{now}θnow​的邻域上距离复杂的目标函数J(θ)J(\\theta)J(θ)很近的简单函数L(θ∣θnow)L(\\theta\\vert\\theta_\\text{now})L(θ∣θnow​)，在邻域中寻找θ\\thetaθ的值最大化L(θ∣θnow)L(\\theta\\vert\\theta_\\text{now})L(θ∣θnow​)并更新参数。 (TRPO中的置信域算法) L(θ∣θnow)=ES[EA∼π(⋅∣s;θnow)[π(A∣S;θ)π(A∣S;θnow)⋅Qπ(s,A)]],L(\\theta\\vert\\theta_\\text{now})=\\mathbb{E}_S\\left[\\mathbb{E}_{A\\sim\\pi(\\cdot\\vert s;\\theta_\\text{now})}\\left[\\dfrac{\\pi(A\\vert S;\\theta)}{\\pi(A\\vert S;\\theta_\\text{now})}\\cdot Q^\\pi(s,A)\\right]\\right],L(θ∣θnow​)=ES​[EA∼π(⋅∣s;θnow​)​[π(A∣S;θnow​)π(A∣S;θ)​⋅Qπ(s,A)]],并进一步使用蒙特卡洛模拟(两步近似Qπ→Qπnow→utQ^\\pi\\to Q^{\\pi_\\text{now}}\\to u_tQπ→Qπnow​→ut​)转化成 L~(θ∣θnow)=1n∑t=1nπ(st,at;θ)π(st,at;θnow)⋅ut.\\tilde{L}(\\theta\\vert\\theta_\\text{now})=\\dfrac{1}{n}\\sum\\limits_{t=1}^n\\dfrac{\\pi(s_t,a_t;\\theta)}{\\pi(s_t,a_t;\\theta_\\text{now})}\\cdot u_t.L~(θ∣θnow​)=n1​t=1∑n​π(st​,at​;θnow​)π(st​,at​;θ)​⋅ut​.在最大化时，可以选择欧几里得距离选取置信域，也可以用KL散度 max⁡θL~(θ∣θnow),s.t. 1n∑t=1nDKL(π(⋅∣st;θnow)∥π(⋅∣st;θ))≤Δ,\\max\\limits_{\\theta}\\tilde{L}(\\theta\\vert\\theta_\\text{now}),\\quad \\text{s.t. }\\dfrac{1}{n}\\sum\\limits_{t=1}^nD_\\text{KL}\\left(\\pi(\\cdot\\vert s_t;\\theta_\\text{now})\\Vert\\pi(\\cdot\\vert s_t;\\theta)\\right)\\leq\\Delta,θmax​L~(θ∣θnow​),s.t. n1​t=1∑n​DKL​(π(⋅∣st​;θnow​)∥π(⋅∣st​;θ))≤Δ,这是一个约束优化问题，代码实现较为复杂。一些文献使用优势函数Aπ(S,A)=Qπ(S,A)−Vπ(S)A^\\pi(S,A)=Q^\\pi(S,A)-V^\\pi(S)Aπ(S,A)=Qπ(S,A)−Vπ(S)替换Qπ(s,a)Q^\\pi(s,a)Qπ(s,a)，然后使用GAE计算优势值，注意有Aπ(s,a)=Es′(r(s,a)+γVπ(s′)−Vπ(s))A^\\pi(s,a)=\\mathbb{E}_{s^\\prime}(r(s,a)+\\gamma V^\\pi(s^\\prime)-V^\\pi(s))Aπ(s,a)=Es′​(r(s,a)+γVπ(s′)−Vπ(s))。 Trust Region Policy Optimization (Schulman et al., ICML, 2015)[Code] ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:4:5","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#trpo-trust-region-policy-optimization"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"PPO (Proximal Policy Optimization) TRPO的改进版，有两种形式 PPO-Penalty: 用拉格朗日乘子法将TRPO中的KL散度约束放进了目标函数中，并在迭代的过程中不断更新KL散度的系数 arg⁡max⁡θESEA∼π(⋅∣S;θnow)[π(A∣S;θ)π(A∣S;θnow)Aπ(S,A)−βDKL(π(⋅∣S;θnow)∥π(⋅∣S;θ))].\\arg\\max\\limits_{\\theta}\\mathbb{E}_S\\mathbb{E}_{A\\sim\\pi(\\cdot\\vert S;\\theta_\\text{now})}\\left[\\dfrac{\\pi(A\\vert S;\\theta)}{\\pi(A\\vert S;\\theta_\\text{now})}A^\\pi(S,A)-\\beta D_\\text{KL}\\left(\\pi(\\cdot\\vert S;\\theta_\\text{now})\\Vert\\pi(\\cdot\\vert S;\\theta)\\right)\\right].argθmax​ES​EA∼π(⋅∣S;θnow​)​[π(A∣S;θnow​)π(A∣S;θ)​Aπ(S,A)−βDKL​(π(⋅∣S;θnow​)∥π(⋅∣S;θ))].当KL散度项小于δ/1.5\\delta/1.5δ/1.5时令β\\betaβ缩小一半，当大于δ/1.5\\delta/1.5δ/1.5时增大一倍，等于时维持现状。 PPO-Clip: arg⁡max⁡θESEA∼π(⋅∣S;θnow)[min⁡{π(A∣S;θ)π(A∣S;θnow)Aπ(S,A),clip(π(A∣S;θ)π(A∣S;θnow),1−ϵ,1+ϵ)Aπ(S,A)}],\\arg\\max\\limits_{\\theta}\\mathbb{E}_S\\mathbb{E}_{A\\sim\\pi(\\cdot\\vert S;\\theta_\\text{now})}\\left[\\min\\left\\{\\dfrac{\\pi(A\\vert S;\\theta)}{\\pi(A\\vert S;\\theta_\\text{now})}A^\\pi(S,A),\\text{clip}\\left(\\dfrac{\\pi(A\\vert S;\\theta)}{\\pi(A\\vert S;\\theta_\\text{now})},1-\\epsilon,1+\\epsilon\\right)A^\\pi(S,A)\\right\\}\\right],argθmax​ES​EA∼π(⋅∣S;θnow​)​[min{π(A∣S;θnow​)π(A∣S;θ)​Aπ(S,A),clip(π(A∣S;θnow​)π(A∣S;θ)​,1−ϵ,1+ϵ)Aπ(S,A)}],其中clip(x,l,r)≔max⁡{min⁡{x,r},l}\\text{clip}(x,l,r)\\coloneqq\\max\\left\\{\\min\\left\\{x,r\\right\\},l\\right\\}clip(x,l,r):=max{min{x,r},l}，ϵ\u003e0\\epsilon\u003e0ϵ\u003e0是一个超参数，当优势大于0时，说明动作价值高于平均，最大化这个式子会增大比值，但不会超过1+ϵ1+\\epsilon1+ϵ；当优势小于0时，说明动作价值低于平均，则最大化u这个式子会缩小比值，但不会低于1−ϵ1-\\epsilon1−ϵ。 大量的实验表明PPO-Clip比PPO-Penalty更好。简单来说，PPO-Clip可以理解为对于好动作（优势值大于0），提升策略生成动作的概率，但是不能偏离基准策略太远；对于坏动作（优势值小于0），降低策略生成动作的概率，但是不能偏离基准策略太远。 Proximal Policy Optimization Algorithms (Schulman et al., 2017)[Code] ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:4:6","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#ppo-proximal-policy-optimization"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"DDPG (Deep Deterministic Policy Gradient) 使用（确定性）策略网络μ(s;θ)\\mu(s;\\theta)μ(s;θ)学习最优策略，目标定义为 J(θ)=ES[q(S,μ(S;θ);ω)],J(\\theta)=\\mathbb{E}_S\\left[q(S,\\mu(S;\\theta);\\omega)\\right],J(θ)=ES​[q(S,μ(S;θ);ω)],其中价值网络使用TD算法进行更新，策略网络通过最大化当前的价值网络进行更新。注意DDPG的价值网络近似动作价值函数而非最优动作价值函数；在选择动作时，一般会加一个噪声ϵ\\epsilonϵ。DDPG中也存在最大化（当前的价值网络）和自举造成的高估问题。 Continuous control with deep reinforcement learning (DeepMind, 2015)[Code] ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:4:7","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#ddpg-deep-deterministic-policy-gradient"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"TD3 (Twin Delayed Deep Deterministic Gradient) 针对DDPG的高估问题，使用两个价值网络和一个策略网络，对应三个目标网络q(s,a;ω1−),q(s,a;ω2−),μ(s;θ−)q(s,a;\\omega_1^-),q(s,a;\\omega_2^-),\\mu(s;\\theta^-)q(s,a;ω1−​),q(s,a;ω2−​),μ(s;θ−)，在计算TD target时，策略目标网络输出动作向量，然后取两个价值目标网络输出的较小值作为TD target。此外，TD3还有一些细节: (动作噪声) 在计算TD target的输出动作步骤中，加入各元素从截断正态分布CN(0,σ2,−c,c)\\mathcal{CN}(0,\\sigma^2,-c,c)CN(0,σ2,−c,c)(防止噪声过大)中采样的噪声向量；(更新频率) 减小更新策略网络和目标网络的频率。 Addressing Function Approximation Error in Actor-Critic Methods (Fujimoto et al., ICML, 2018)[Code] ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:4:8","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#td3-twin-delayed-deep-deterministic-gradient"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"随机高斯策略 (Stochastic Gaussian Policy) 使用两个神经网络分别近似高斯分布的均值和对数方差，利用重参数化技巧进行动作采样，并用策略梯度更新两个神经网络的参数。 随机高斯策略最早出现在Reinforce (Williams, 1992) 的论文中，作为处理连续动作空间（Continuous Action Spaces）的标准方法。Williams 在文中明确推导了当策略网络输出高斯分布的均值（μ\\muμ）和标准差（σ\\sigmaσ）时的梯度更新公式。(Gemini说的,不保真) ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:4:9","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#随机高斯策略-stochastic-gaussian-policy"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"SAC(Soft Actor-Critic) SAC的前身是Soft Q-learning，都属于最大熵强化学习，Soft Q-learning 不存在一个显式的策略函数，而是使用一个函数的波尔兹曼分布，在连续空间下求解非常麻烦。于是 SAC 提出使用一个 Actor 表示策略函数，从而解决这个问题。SAC使用了两个Critic网络和对应的目标网络，在计算计算TD目标和策略更新时都取两个网络的最小值。由于在学习目标中加入了熵正则项，因此TD目标和策略更新都多了一个log pi(s\\vert a)项。注意SAC输出的是随机高斯策略。 Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor (Haarnoja, ICML, 2018)[Code] ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:4:10","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#sacsoft-actor-critic"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"连续控制问题 在连续动作向量的问题中，一个自然的想法是通过离散化动作空间然后训练DQN或者策略网络，但是随着自由度的增大，离散后的动作空间的大小指数增长，会造成维数灾难，DQN和策略网络的训练都变得异常困难。在真实的环境中，虽然动作空间可能是离散的，但是可能动作空间非常庞大（比如推荐系统的item数量），往往会输出连续动作向量再映射到有效动作空间（比如item列表），DDPG、TD3就是处理连续动作空间的有效办法。 ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:5:0","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#连续控制问题"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"同策略(On-Policy)和异策略(Off-Policy) 定义: 首先引入行为策略(behavior policy)和目标策略(target policy)的概念，行为策略是与环境交互收集经验的策略，训练时的策略称为目标策略，如果行为策略和目标策略相同，称为同策略，如果不同则称为异策略。 常见算法分类: 常见的同策略方法包括Reinforce(收集utu_tut​和用于训练的策略是同一策略)、Reinforce with baseline(同理)、Actor-Critic(使用SARSA算法训练，且策略梯度是根据目标策略计算的)、A2C(没有经验回放池)、TRPO(与A2C同理)；常见的异策略方法包括DQN及其变体(使用经验回放数组)、DDPG(同理)、TD3(同理)、SAC ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:6:0","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#同策略on-policy和异策略off-policy"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"代码实现细节 目标网络: 通常目标网络的参数不加入优化器，而是定期根据价值网络或策略网络的参数进行优化； 随机策略: 在策略梯度的算法中，如果使用的是随机策略，一般用神经网络输出logits再经过softmax得到每个动作的概率，在选择动作时使用一个类别分布进行抽样得到动作 ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:7:0","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#代码实现细节"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"未完待续 PPO中使用了两次近似，后续会详细阐述。 ","date":"2026-01-15","objectID":"/posts/iii-policy-based-rl/:8:0","tags":["RL"],"title":"III Policy-Based RL","uri":"/posts/iii-policy-based-rl/#未完待续"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"本文梳理了基于价值的强化学习算法。","date":"2026-01-10","objectID":"/posts/ii-value-based-rl/","tags":["RL"],"title":"II Value-Based RL","uri":"/posts/ii-value-based-rl/"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"基本概念 价值学习方法关注如何近似最优价值函数Q∗(s,a)Q^\\ast(s,a)Q∗(s,a)，使用π(a∣s)=arg⁡max⁡aQ∗(s,a)\\pi(a\\vert s)=\\arg\\max\\limits_{a}Q^\\ast(s,a)π(a∣s)=argamax​Q∗(s,a)作为最优策略。 ","date":"2026-01-10","objectID":"/posts/ii-value-based-rl/:1:0","tags":["RL"],"title":"II Value-Based RL","uri":"/posts/ii-value-based-rl/#基本概念"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"常见算法 ","date":"2026-01-10","objectID":"/posts/ii-value-based-rl/:2:0","tags":["RL"],"title":"II Value-Based RL","uri":"/posts/ii-value-based-rl/#常见算法"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"DQN 通过最优贝尔曼方程和（期望的）蒙特卡洛近似推导出TD算法训练。 Playing Atari with Deep Reinforcement Learning (DeepMind, 2013)[Code] ","date":"2026-01-10","objectID":"/posts/ii-value-based-rl/:2:1","tags":["RL"],"title":"II Value-Based RL","uri":"/posts/ii-value-based-rl/#dqn"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"Double DQN 在DQN的基础上引入目标网络缓解（由max操作和自举导致的）高估问题。 Deep Reinforcement Learning with Double Q-learning (DeepMind, AAAI, 2016)[Code] ","date":"2026-01-10","objectID":"/posts/ii-value-based-rl/:2:2","tags":["RL"],"title":"II Value-Based RL","uri":"/posts/ii-value-based-rl/#double-dqn"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"Dueling DQN 通过优势头A(s,a;ωA)A(s,a;\\omega^A)A(s,a;ωA)和状态价值头V(s;ωV)V(s;\\omega^V)V(s;ωV)近似最优价值函数，也可以使用目标网络的技术缓解高估问题 Dueling Network Architectures for Deep Reinforcement Learning (Google, ICLR, 2016) [Code] ","date":"2026-01-10","objectID":"/posts/ii-value-based-rl/:2:3","tags":["RL"],"title":"II Value-Based RL","uri":"/posts/ii-value-based-rl/#dueling-dqn"},{"categories":["Basic Knowledge"],"collections":["Reinforcement Learning"],"content":"本文主要梳理马尔可夫决策过程中的基本概念以及贝尔曼方程和最优贝尔曼方程。","date":"2026-01-10","objectID":"/posts/i-preliminaries/","tags":["RL"],"title":"I Preliminaries","uri":"/posts/i-preliminaries/"},{"categories":["Basic Knowledge"],"collections":["Reinforcement Learning"],"content":"基本概念 ","date":"2026-01-10","objectID":"/posts/i-preliminaries/:1:0","tags":["RL"],"title":"I Preliminaries","uri":"/posts/i-preliminaries/#基本概念"},{"categories":["Basic Knowledge"],"collections":["Reinforcement Learning"],"content":"折扣回报 (discounted return) 定义为从当前时刻的状态开始到终止状态结束时所有奖励的之和，考虑到未来奖励的不确定性，引入了折扣因子γ\\gammaγ Ut=Rt+γRt+1+γ2Rt+2+⋯=∑k=0γkRt+k,U_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\cdots=\\sum\\limits_{k=0}\\gamma^kR_{t+k},Ut​=Rt​+γRt+1​+γ2Rt+2​+⋯=k=0∑​γkRt+k​,注意UtU_tUt​是一个随机变量，依赖于未来的状态和动作。 ","date":"2026-01-10","objectID":"/posts/i-preliminaries/:1:1","tags":["RL"],"title":"I Preliminaries","uri":"/posts/i-preliminaries/#折扣回报-discounted-return"},{"categories":["Basic Knowledge"],"collections":["Reinforcement Learning"],"content":"状态价值函数(state value function)和动作价值函数(state-action value function) 状态价值函数衡量在策略π\\piπ下，当前状态的期望回报(衡量状态的好坏) Vπ(s)=Eπ[Ut∣St=s].V^\\pi(s)=\\mathbb{E}_\\pi\\left[U_t\\vert S_t=s\\right].Vπ(s)=Eπ​[Ut​∣St​=s].动作价值函数衡量在策略π\\piπ下，当前状态下采取某种动作的期望回报(衡量动作的好坏) Qπ(s,a)=Eπ[Ut∣St=s,At=a].Q^\\pi(s,a)=\\mathbb{E}_\\pi\\left[U_t\\vert S_t=s,A_t=a\\right].Qπ(s,a)=Eπ​[Ut​∣St​=s,At​=a].不难发现状态价值函数和动作价值函数存在如下关系 Vπ(s)=∑a∈Aπ(a∣s)Qπ(s,a),Qπ(s,a)=r(s,a)+γ∑s′∈SP(s′∣s,a)Vπ(s′),\\begin{align}\u0026V^\\pi(s)=\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)Q^\\pi(s,a),\\\\\u0026Q^\\pi(s,a)=r(s,a)+\\gamma\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)V^\\pi(s^\\prime),\\end{align}​Vπ(s)=a∈A∑​π(a∣s)Qπ(s,a),Qπ(s,a)=r(s,a)+γs′∈S∑​P(s′∣s,a)Vπ(s′),​​其中r(s,a)r(s,a)r(s,a)是环境的奖励函数，P(s′∣s,a)P(s^\\prime\\vert s,a)P(s′∣s,a)是环境的状态转移函数，共同构成了环境动态。 ","date":"2026-01-10","objectID":"/posts/i-preliminaries/:1:2","tags":["RL"],"title":"I Preliminaries","uri":"/posts/i-preliminaries/#状态价值函数state-value-function和动作价值函数state-action-value-function"},{"categories":["Basic Knowledge"],"collections":["Reinforcement Learning"],"content":"贝尔曼方程(Bellman equation) 根据价值函数之间的关系，不难推导出两个价值函数的贝尔曼方程 Vπ(s)=∑a∈Aπ(a∣s)[r(s,a)+γ∑s′∈SP(s′∣s,a)Vπ(s′)],Qπ(s,a)=r(s,a)+γ∑s′∈SP(s′∣s,a)∑a′∈Aπ(a′∣s′)Qπ(s′,a′),\\begin{align}\u0026V^\\pi(s)=\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)\\left[r(s,a)+\\gamma\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)V^\\pi(s^\\prime)\\right],\\\\\u0026Q^\\pi(s,a)=r(s,a)+\\gamma\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\sum\\limits_{a^\\prime\\in\\mathcal{A}}\\pi(a^\\prime\\vert s^\\prime)Q^\\pi(s^\\prime,a^\\prime),\\end{align}​Vπ(s)=a∈A∑​π(a∣s)[r(s,a)+γs′∈S∑​P(s′∣s,a)Vπ(s′)],Qπ(s,a)=r(s,a)+γs′∈S∑​P(s′∣s,a)a′∈A∑​π(a′∣s′)Qπ(s′,a′),​​为了和TD learning对应，贝尔曼方程可以改写为如下形式 Vπ(s)=∑a∈Aπ(a∣s)[∑s′∈SP(s′∣s,a)[r(s,a)+γVπ(s′)]],Qπ(s,a)=∑s′∈SP(s′∣s,a)[r(s,a)+γ∑a′∈Aπ(a′∣s′)Qπ(s′,a′)],\\begin{align}\u0026V^\\pi(s)=\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)\\left[\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\left[r(s,a)+\\gamma V^\\pi(s^\\prime)\\right]\\right],\\\\\u0026Q^\\pi(s,a)=\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\left[r(s,a)+\\gamma\\sum\\limits_{a^\\prime\\in\\mathcal{A}}\\pi(a^\\prime\\vert s^\\prime)Q^\\pi(s^\\prime,a^\\prime)\\right],\\end{align}​Vπ(s)=a∈A∑​π(a∣s)[s′∈S∑​P(s′∣s,a)[r(s,a)+γVπ(s′)]],Qπ(s,a)=s′∈S∑​P(s′∣s,a)[r(s,a)+γa′∈A∑​π(a′∣s′)Qπ(s′,a′)],​​","date":"2026-01-10","objectID":"/posts/i-preliminaries/:1:3","tags":["RL"],"title":"I Preliminaries","uri":"/posts/i-preliminaries/#贝尔曼方程bellman-equation"},{"categories":["Basic Knowledge"],"collections":["Reinforcement Learning"],"content":"最优状态价值函数和最优动作价值函数 考虑策略空间Π\\PiΠ中的偏序关系: 当且仅当对于任意状态s∈Ss\\in\\mathcal{S}s∈S都有Vπ(s)≥Vπ′(s)V^\\pi(s)\\geq V^{\\pi^\\prime}(s)Vπ(s)≥Vπ′(s)，记π≻π′\\pi\\succ\\pi^\\primeπ≻π′。假设存在最优策略（比其他所有策略都好或者不差于其他所有策略），记为π∗\\pi^\\astπ∗，定义最优价值函数 V∗(s)=max⁡πVπ(s),V^\\ast(s)=\\max\\limits_{\\pi}V^\\pi(s),V∗(s)=πmax​Vπ(s),以及最优动作价值函数 Q∗(s,a)=max⁡πQπ(s,a).Q^\\ast(s,a)=\\max\\limits_{\\pi}Q^\\pi(s,a).Q∗(s,a)=πmax​Qπ(s,a).考虑到为了使得Qπ(s,a)Q^\\pi(s,a)Qπ(s,a)最大需要在(s,a)(s,a)(s,a)之后都执行最优策略,由最优策略的定义可得 Q∗(s,a)=r(s,a)+γ∑s′∈SP(s′∣s,a)V∗(s′),Q^\\ast(s,a)=r(s,a)+\\gamma\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)V^\\ast(s^\\prime),Q∗(s,a)=r(s,a)+γs′∈S∑​P(s′∣s,a)V∗(s′),这与普通策略下的动作价值函数和状态价值函数之间的关系是一样的，此外考虑到最优策略需要在任意状态都需要执行，因此有 V∗(s)=max⁡aQ∗(s,a).V^\\ast(s)=\\max\\limits_{a}Q^\\ast(s,a).V∗(s)=amax​Q∗(s,a).","date":"2026-01-10","objectID":"/posts/i-preliminaries/:1:4","tags":["RL"],"title":"I Preliminaries","uri":"/posts/i-preliminaries/#最优状态价值函数和最优动作价值函数"},{"categories":["Basic Knowledge"],"collections":["Reinforcement Learning"],"content":"贝尔曼最优方程(Bellman optimality equation) 由上述最优状态价值函数和最优动作价值函数的关系，不难得到贝尔曼最优方程 V∗(s)=max⁡a∈A(r(s,a)+∑s′∈SP(s′∣s,a)V∗(s′))Q∗(s,a)=r(s,a)+γ∑s′∈SP(s′∣s,a)max⁡a∈AQ∗(s′,a′)\\begin{align}\u0026V^\\ast(s)=\\max\\limits_{a\\in\\mathcal{A}}\\left(r(s,a)+\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)V^\\ast(s^\\prime)\\right)\\\\\u0026Q^\\ast(s,a)=r(s,a)+\\gamma\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\max\\limits_{a\\in\\mathcal{A}}Q^\\ast(s^\\prime,a^\\prime)\\end{align}​V∗(s)=a∈Amax​(r(s,a)+s′∈S∑​P(s′∣s,a)V∗(s′))Q∗(s,a)=r(s,a)+γs′∈S∑​P(s′∣s,a)a∈Amax​Q∗(s′,a′)​​ 为了和TD learning对应，贝尔曼最优方程可以改写为如下形式 V∗(s)=max⁡a∈A[∑s′∈SP(s′∣s,a)[r(s,a)+V∗(s′)]]Q∗(s,a)=∑s′∈SP(s′∣s,a)[r(s,a)+γmax⁡a∈AQ∗(s′,a′)]\\begin{align} \u0026V^\\ast(s)=\\max\\limits_{a\\in\\mathcal{A}}\\left[\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\left[r(s,a)+V^\\ast(s^\\prime)\\right]\\right]\\\\\u0026Q^\\ast(s,a)=\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\left[r(s,a)+\\gamma\\max\\limits_{a\\in\\mathcal{A}}Q^\\ast(s^\\prime,a^\\prime)\\right] \\end{align}​V∗(s)=a∈Amax​[s′∈S∑​P(s′∣s,a)[r(s,a)+V∗(s′)]]Q∗(s,a)=s′∈S∑​P(s′∣s,a)[r(s,a)+γa∈Amax​Q∗(s′,a′)]​​注: 以上最优策略的定义是定义在finit MDP中的，详见Sutton《Reinforcement learning: an introduction》- 3.6 Optimal Policies and Optimal Value Functions. ","date":"2026-01-10","objectID":"/posts/i-preliminaries/:1:5","tags":["RL"],"title":"I Preliminaries","uri":"/posts/i-preliminaries/#贝尔曼最优方程bellman-optimality-equation"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":" 整理这些内容的初衷是在实习面试前过一遍强化学习，大概花了几天的时间梳理了传统RL的算法，发现很多地方理解不够深刻，所以也对之前一些模棱两可的理解进一步查阅了资料。 我的强化学习是通过王树森老师的深度强化学习在线课程学习的,其实没有系统性地阅读各个算法的原文,王老师的课程风格简约、深入浅出,很适合初学者。不过在线课程能展示的内容有限，很多细节还是需要通过阅读文字资料才能进一步掌握。 I-III的内容基本来自于王树森老师的《深度强化学习》和上交张伟楠老师、俞勇老师合著的《动手学强化学习》，IV及之后的内容整理了我知识范围内能接触到的比较有影响力的RL算法。 整体内容偏知识回顾和总结，不强调算法细节。 ","date":"2026-01-09","objectID":"/posts/%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2/:0:0","tags":["RL"],"title":"写在前面","uri":"/posts/%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2/#"}]