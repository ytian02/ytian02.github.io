[{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"本文梳理了基于价值的强化学习算法。","date":"2026-01-10","objectID":"/posts/value-based-rl/","tags":["RL"],"title":"Value Based RL","uri":"/posts/value-based-rl/"},{"categories":["Algorithms"],"collections":["Reinforcement Learning"],"content":"价值学习方法关注如何近似最优价值函数Q∗(s,a)Q^\\ast(s,a)Q∗(s,a)，使用π(a∣s)=arg⁡max⁡aQ∗(s,a)\\pi(a\\vert s)=\\arg\\max\\limits_{a}Q^\\ast(s,a)π(a∣s)=argamax​Q∗(s,a)作为最优策略。 DQN: 通过最优贝尔曼方程和（期望的）蒙特卡洛近似推导出TD算法训练 Double DQN: 在DQN的基础上引入目标网络缓解（由max操作和自举导致的）高估问题 Dueling DQN: 通过优势头A(s,a;ωA)A(s,a;\\omega^A)A(s,a;ωA)和状态价值头V(s;ωV)V(s;\\omega^V)V(s;ωV)近似最优价值函数，也可以使用目标网络的技术缓解高估问题 ","date":"2026-01-10","objectID":"/posts/value-based-rl/:0:0","tags":["RL"],"title":"Value Based RL","uri":"/posts/value-based-rl/#"},{"categories":["Basic Knowledge"],"collections":["Reinforcement Learning"],"content":"本文主要梳理马尔可夫决策过程中的基本概念以及贝尔曼方程和最优贝尔曼方程。","date":"2026-01-10","objectID":"/posts/preliminaries/","tags":["RL"],"title":"Preliminaries","uri":"/posts/preliminaries/"},{"categories":["Basic Knowledge"],"collections":["Reinforcement Learning"],"content":"折扣回报(discounted return): 定义了从当前时刻的状态开始到终止状态结束时所有奖励的之和，考虑到未来奖励的不确定性，引入了折扣因子γ\\gammaγ Ut=Rt+γRt+1+γ2Rt+2+⋯=∑k=0γkRt+k,U_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\cdots=\\sum\\limits_{k=0}\\gamma^kR_{t+k},Ut​=Rt​+γRt+1​+γ2Rt+2​+⋯=k=0∑​γkRt+k​,注意UtU_tUt​是一个随机变量，依赖于未来的状态和动作。 状态价值函数(state value function)和动作价值函数(state-action value function): 状态价值函数衡量在策略π\\piπ下，当前状态的期望回报(衡量状态的好坏) Vπ(s)=Eπ[Ut∣St=s].V^\\pi(s)=\\mathbb{E}_\\pi\\left[U_t\\vert S_t=s\\right].Vπ(s)=Eπ​[Ut​∣St​=s].动作价值函数衡量在策略π\\piπ下，当前状态下采取某种动作的期望回报(衡量动作的好坏) Qπ(s,a)=Eπ[Ut∣St=s,At=a].Q^\\pi(s,a)=\\mathbb{E}_\\pi\\left[U_t\\vert S_t=s,A_t=a\\right].Qπ(s,a)=Eπ​[Ut​∣St​=s,At​=a].不难发现状态价值函数和动作价值函数存在如下关系 Vπ(s)=∑a∈Aπ(a∣s)Qπ(s,a),Qπ(s,a)=r(s,a)+γ∑s′∈SP(s′∣s,a)Vπ(s′),\\begin{align}\u0026V^\\pi(s)=\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)Q^\\pi(s,a),\\\\\u0026Q^\\pi(s,a)=r(s,a)+\\gamma\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)V^\\pi(s^\\prime),\\end{align}​Vπ(s)=a∈A∑​π(a∣s)Qπ(s,a),Qπ(s,a)=r(s,a)+γs′∈S∑​P(s′∣s,a)Vπ(s′),​​其中r(s,a)r(s,a)r(s,a)是环境的奖励函数，P(s′∣s,a)P(s^\\prime\\vert s,a)P(s′∣s,a)是环境的状态转移函数，共同构成了环境动态。 贝尔曼方程(Bellman equation): 不难推导出两个价值函数的贝尔曼方程 Vπ(s)=∑a∈Aπ(a∣s)[r(s,a)+γ∑s′∈SP(s′∣s,a)Vπ(s′)],Qπ(s,a)=r(s,a)+γ∑s′∈SP(s′∣s,a)∑a′∈Aπ(a′∣s′)Qπ(s′,a′),\\begin{align}\u0026V^\\pi(s)=\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)\\left[r(s,a)+\\gamma\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)V^\\pi(s^\\prime)\\right],\\\\\u0026Q^\\pi(s,a)=r(s,a)+\\gamma\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\sum\\limits_{a^\\prime\\in\\mathcal{A}}\\pi(a^\\prime\\vert s^\\prime)Q^\\pi(s^\\prime,a^\\prime),\\end{align}​Vπ(s)=a∈A∑​π(a∣s)[r(s,a)+γs′∈S∑​P(s′∣s,a)Vπ(s′)],Qπ(s,a)=r(s,a)+γs′∈S∑​P(s′∣s,a)a′∈A∑​π(a′∣s′)Qπ(s′,a′),​​为了和TD learning对应，贝尔曼方程可以改写为如下形式 Vπ(s)=∑a∈Aπ(a∣s)[∑s′∈SP(s′∣s,a)[r(s,a)+γVπ(s′)]],Qπ(s,a)=∑s′∈SP(s′∣s,a)[r(s,a)+γ∑a′∈Aπ(a′∣s′)Qπ(s′,a′)],\\begin{align}\u0026V^\\pi(s)=\\sum\\limits_{a\\in\\mathcal{A}}\\pi(a\\vert s)\\left[\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\left[r(s,a)+\\gamma V^\\pi(s^\\prime)\\right]\\right],\\\\\u0026Q^\\pi(s,a)=\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\left[r(s,a)+\\gamma\\sum\\limits_{a^\\prime\\in\\mathcal{A}}\\pi(a^\\prime\\vert s^\\prime)Q^\\pi(s^\\prime,a^\\prime)\\right],\\end{align}​Vπ(s)=a∈A∑​π(a∣s)[s′∈S∑​P(s′∣s,a)[r(s,a)+γVπ(s′)]],Qπ(s,a)=s′∈S∑​P(s′∣s,a)[r(s,a)+γa′∈A∑​π(a′∣s′)Qπ(s′,a′)],​​最优状态价值函数和最优动作价值函数: 考虑策略空间Π\\PiΠ中的偏序关系: 当且仅当对于任意状态s∈Ss\\in\\mathcal{S}s∈S都有Vπ(s)≥Vπ′(s)V^\\pi(s)\\geq V^{\\pi^\\prime}(s)Vπ(s)≥Vπ′(s)，记π≻π′\\pi\\succ\\pi^\\primeπ≻π′。假设存在最优策略（比其他所有策略都好或者不差于其他所有策略），记为π∗\\pi^\\astπ∗，定义最优价值函数 V∗(s)=max⁡πVπ(s),V^\\ast(s)=\\max\\limits_{\\pi}V^\\pi(s),V∗(s)=πmax​Vπ(s),以及最优动作价值函数 Q∗(s,a)=max⁡πQπ(s,a).Q^\\ast(s,a)=\\max\\limits_{\\pi}Q^\\pi(s,a).Q∗(s,a)=πmax​Qπ(s,a).考虑到为了使得Qπ(s,a)Q^\\pi(s,a)Qπ(s,a)最大需要在(s,a)(s,a)(s,a)之后都执行最优策略,由最优策略的定义可得 Q∗(s,a)=r(s,a)+γ∑s′∈SP(s′∣s,a)V∗(s′),Q^\\ast(s,a)=r(s,a)+\\gamma\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)V^\\ast(s^\\prime),Q∗(s,a)=r(s,a)+γs′∈S∑​P(s′∣s,a)V∗(s′),这与普通策略下的动作价值函数和状态价值函数之间的关系是一样的，此外考虑到最优策略需要在任意状态都需要执行，因此有 V∗(s)=max⁡aQ∗(s,a).V^\\ast(s)=\\max\\limits_{a}Q^\\ast(s,a).V∗(s)=amax​Q∗(s,a).贝尔曼最优方程(Bellman optimality equation): 由上述最优状态价值函数和最优动作价值函数的关系，不难得到贝尔曼最优方程 V∗(s)=max⁡a∈A(r(s,a)+∑s′∈SP(s′∣s,a)V∗(s′))Q∗(s,a)=r(s,a)+γ∑s′∈SP(s′∣s,a)max⁡a∈AQ∗(s′,a′)\\begin{align}\u0026V^\\ast(s)=\\max\\limits_{a\\in\\mathcal{A}}\\left(r(s,a)+\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)V^\\ast(s^\\prime)\\right)\\\\\u0026Q^\\ast(s,a)=r(s,a)+\\gamma\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\max\\limits_{a\\in\\mathcal{A}}Q^\\ast(s^\\prime,a^\\prime)\\end{align}​V∗(s)=a∈Amax​(r(s,a)+s′∈S∑​P(s′∣s,a)V∗(s′))Q∗(s,a)=r(s,a)+γs′∈S∑​P(s′∣s,a)a∈Amax​Q∗(s′,a′)​​ 为了和TD learning对应，贝尔曼最优方程可以改写为如下形式 V∗(s)=max⁡a∈A[∑s′∈SP(s′∣s,a)[r(s,a)+V∗(s′)]]Q∗(s,a)=∑s′∈SP(s′∣s,a)[r(s,a)+γmax⁡a∈AQ∗(s′,a′)]\\begin{align} \u0026V^\\ast(s)=\\max\\limits_{a\\in\\mathcal{A}}\\left[\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\left[r(s,a)+V^\\ast(s^\\prime)\\right]\\right]\\\\\u0026Q^\\ast(s,a)=\\sum\\limits_{s^\\prime\\in\\mathcal{S}}P(s^\\prime\\vert s,a)\\left[r(s,a)+\\gamma\\max\\limits_{a\\in\\mathcal{A}}Q^\\ast(s^\\prime,a^\\prime)\\right] \\end{align","date":"2026-01-10","objectID":"/posts/preliminaries/:0:0","tags":["RL"],"title":"Preliminaries","uri":"/posts/preliminaries/#"}]